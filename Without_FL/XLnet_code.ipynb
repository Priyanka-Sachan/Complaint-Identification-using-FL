{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "XLnet_code",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanka-Sachan/Complaint-Identification-using-FL/blob/master/Without_FL/XLnet_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "from transformers import AdamW\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from transformers import get_scheduler\n",
        "from datasets import load_metric\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH1fDxSVmGsP"
      },
      "source": [
        "# For Reproducibility\n",
        "SEED=9\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlJ02Pvwf55f"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXd5-KDk_PX5"
      },
      "source": [
        "##Reading and Pre-processing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ"
      },
      "source": [
        "# Load the .csv file.\n",
        "data = pd.read_csv(\"complaints-data.csv\", header=None, names=['id', 'tweet', 'y', 'industry'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcEAbCxiZOiT"
      },
      "source": [
        "# Tokenize sentences\n",
        "sentences=data.tweet.values\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "MAX_LEN=49\n",
        "tokens= [tokenizer(sentence, padding='max_length',truncation=\"only_first\", max_length=MAX_LEN) for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeKuiqNao777"
      },
      "source": [
        "# Get input ids, attention masks and labels.\n",
        "input_ids=np.asarray([np.asarray(token['input_ids']) for token in tokens])\n",
        "attention_masks=np.asarray([np.asarray(token['attention_mask']) for token in tokens])\n",
        "labels=data.y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLIa-SUF_Yuo"
      },
      "source": [
        "##Training and Testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX47HxYb98ka"
      },
      "source": [
        "# Function to  create dataloader\n",
        "def create_dataloader(input_ids,masks,labels):\n",
        "\n",
        "  input_ids=torch.tensor(input_ids)\n",
        "  masks=torch.tensor(masks)\n",
        "  labels=torch.tensor(labels)\n",
        "\n",
        "  data = TensorDataset(input_ids,masks,labels)\n",
        "  sampler = RandomSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=32) \n",
        "\n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv029I3s5pN3"
      },
      "source": [
        "# Function to train and validate a model\n",
        "def train_and_validate_model(learning_rate,train_dataloader,validation_dataloader): \n",
        " \n",
        "    model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    num_epochs = 4\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "    early_stopping = EarlyStopping(patience=3, verbose=False)\n",
        " \n",
        "    train_accuracy_metric=load_metric(\"accuracy\")\n",
        "    valid_accuracy_metric=load_metric(\"accuracy\")\n",
        " \n",
        "    train_loss,valid_loss=0,0\n",
        "    pr_train_loss,pr_valid_loss=0,0\n",
        " \n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      train_losses = []\n",
        "      valid_losses = []\n",
        " \n",
        "      model.train()\n",
        "      for batch in train_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          outputs = model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "          loss = outputs.loss\n",
        "          logits = outputs.logits\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        " \n",
        "          train_losses.append(loss.item())\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          train_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        " \n",
        "      model.eval()\n",
        "      for batch in validation_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          with torch.no_grad():\n",
        "              outputs = model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "          loss = outputs.loss\n",
        "          logits = outputs.logits\n",
        " \n",
        "          valid_losses.append(loss.item())\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          valid_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "      pr_train_loss=train_loss\n",
        "      train_loss = np.average(train_losses)\n",
        "      pr_valid_loss=valid_loss\n",
        "      valid_loss = np.average(valid_losses)\n",
        " \n",
        "      early_stopping(valid_loss, model)\n",
        "          \n",
        "      if early_stopping.early_stop:\n",
        "          print(\"Early stopping\")\n",
        "          valid_loss=pr_valid_loss\n",
        "          train_loss=pr_train_loss\n",
        "          break\n",
        "\n",
        "      train_accuracy=train_accuracy_metric.compute()['accuracy']\n",
        "      valid_accuracy=valid_accuracy_metric.compute()['accuracy']\n",
        "\n",
        "      print(\"EPOCH: {}\".format(epoch+1),\n",
        "            \"| Train accuracy: {:7.5f}\".format(train_accuracy),\n",
        "            \"| Train loss: {:7.5f}\".format(train_loss),\n",
        "            \"| Validation accuracy: {:7.5f}\".format(valid_accuracy),\n",
        "            \"| Validation loss: {:7.5f}\".format(valid_loss))\n",
        " \n",
        "    return train_accuracy,train_loss,valid_accuracy,valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5butv1ImLHWZ"
      },
      "source": [
        "#Function to train and test a model\n",
        "def train_and_test_model(learning_rate,train_dataloader,test_dataloader): \n",
        " \n",
        "    model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "    model.cuda()\n",
        " \n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    num_epochs = 4\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        " \n",
        "    train_accuracy_metric=load_metric(\"accuracy\")\n",
        "    test_accuracy_metric=load_metric(\"accuracy\")\n",
        "    test_precision_metric=load_metric(\"precision\")\n",
        "    test_recall_metric=load_metric(\"recall\")\n",
        "    test_f1_metric=load_metric(\"f1\")\n",
        "\n",
        "    train_loss,test_loss=0,0\n",
        " \n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      train_losses = []\n",
        "      model.train()\n",
        "      for batch in train_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          outputs = model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "          loss = outputs.loss\n",
        "          logits = outputs.logits\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        " \n",
        "          train_losses.append(loss.item())\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          train_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        "\n",
        "      train_accuracy=train_accuracy_metric.compute()['accuracy']\n",
        "      train_loss = np.average(train_losses)\n",
        "      print(\"EPOCH: {}\".format(epoch+1),\n",
        "            \"| Train accuracy: {:7.5f}\".format(train_accuracy),\n",
        "            \"| Train loss: {:7.5f}\".format(train_loss))\n",
        " \n",
        "    test_losses = []\n",
        "    model.eval()\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        " \n",
        "        test_losses.append(loss.item())\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        test_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        "        test_precision_metric.add_batch(predictions=predictions,references=b_labels)\n",
        "        test_recall_metric.add_batch(predictions=predictions,references=b_labels)\n",
        "        test_f1_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "    test_loss = np.average(test_losses)\n",
        "    test_accuracy=test_accuracy_metric.compute()['accuracy']\n",
        "    test_precision=test_precision_metric.compute()['precision']\n",
        "    test_recall=test_recall_metric.compute()['recall']\n",
        "    test_f1=test_f1_metric.compute()['f1']\n",
        "\n",
        "    print(\"Learning Rate: {}\".format(learning_rate)+\n",
        "    \" | Test accuracy: {:7.5f}\".format(test_accuracy)+\n",
        "    \" | Test loss: {:7.5f}\".format(test_loss)+\n",
        "    \" | Test precision: {:7.5f}\".format(test_precision)+\n",
        "    \" | Test recall: {:7.5f}\".format(test_recall)+\n",
        "    \" | Test f1 score: {:7.5f}\".format(test_f1))\n",
        "\n",
        "    return train_accuracy,train_loss,test_accuracy,test_loss,test_precision,test_recall,test_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qXB7pWggFVs"
      },
      "source": [
        "##MODEL\n",
        "Available at : https://huggingface.co/transformers/_modules/transformers/models/xlnet/modeling_xlnet.html#XLNetForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMcjiQE_eaE"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jPLEaBDmLLy"
      },
      "source": [
        "# Prepare cross validation\n",
        "outer_cv = KFold(n_splits=10, shuffle=True, random_state= SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e6s4fB8mMCy"
      },
      "source": [
        "train_valid_ids=[]\n",
        "test_ids=[]\n",
        "#Split in 10 folds - 9 folds for training+validation set and 1 fold for test set\n",
        "for train_valid_id, test_id in outer_cv.split(input_ids):\n",
        "  train_valid_ids.append(train_valid_id)\n",
        "  test_ids.append(test_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kG17sHGmSnn"
      },
      "source": [
        "# #To save train_valid_set and test_set iterations of outer cross validation\n",
        "# torch.save(train_valid_ids, 'train_valid_ids.pt')\n",
        "# torch.save(test_ids, 'test_ids.pt')\n",
        "# buffer = io.BytesIO()\n",
        "# torch.save(train_valid_ids, buffer)\n",
        "# torch.save(test_ids, buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4i9N2OwmTc4"
      },
      "source": [
        "# #Loading train_valid_set and test_set\n",
        "# with open('train_valid_ids.pt', 'rb') as f:\n",
        "#   buffer = io.BytesIO(f.read())\n",
        "# train_valid_ids=torch.load(buffer)\n",
        "\n",
        "# with open('test_ids.pt', 'rb') as f:\n",
        "#   buffer = io.BytesIO(f.read())\n",
        "# test_ids=torch.load(buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKb8LbB5_hUP"
      },
      "source": [
        "###K-fold Cross Valiadtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6QNMQc9d5d3"
      },
      "source": [
        "# learning_rate=[5e-06,1e-5,2e-5,4e-5]\n",
        "# for lr in learning_rate:\n",
        "#     # FOR CROSS VALIDATION\n",
        "#     train_accuracy,train_loss,test_loss,test_accuracy,test_recall,test_precision,test_f1=[],[],[],[],[],[],[]\n",
        "\n",
        "#     #Split in 10 folds - 9 folds for training+validation set and 1 fold for test set\n",
        "#     for outer_cv_count in range(10):\n",
        "\n",
        "#         train_dataloader=create_dataloader(input_ids[train_valid_ids[outer_cv_count]],\n",
        "#                                               attention_masks[train_valid_ids[outer_cv_count]],\n",
        "#                                               labels[train_valid_ids[outer_cv_count]])\n",
        "\n",
        "#         test_dataloader=create_dataloader(input_ids[test_ids[outer_cv_count]],\n",
        "#                                           attention_masks[test_ids[outer_cv_count]],\n",
        "#                                           labels[test_ids[outer_cv_count]])\n",
        "\n",
        "#         tr_accuracy,tr_loss,ts_accuracy,ts_loss,ts_precision,ts_recall,ts_f1=train_and_test_model(lr,train_dataloader,test_dataloader)    \n",
        "\n",
        "#         train_accuracy.append(tr_accuracy)    \n",
        "#         train_loss.append(tr_loss)\n",
        "#         test_accuracy.append(ts_accuracy)\n",
        "#         test_loss.append(ts_loss)\n",
        "#         test_precision.append(ts_precision)\n",
        "#         test_recall.append(ts_recall)\n",
        "#         test_f1.append(ts_f1)\n",
        "      \n",
        "#     print(\"Train accuracy:\",torch.std_mean(torch.Tensor(train_accuracy)))\n",
        "#     print(\"Train loss:\",torch.std_mean(torch.Tensor(train_loss)))\n",
        "#     print(\"Test accuracy:\",torch.std_mean(torch.Tensor(test_accuracy)))\n",
        "#     print(\"Test loss:\",torch.std_mean(torch.Tensor(test_loss)))\n",
        "#     print(\"Test precision:\",torch.std_mean(torch.Tensor(test_precision)))\n",
        "#     print(\"Test recall:\",torch.std_mean(torch.Tensor(test_recall)))\n",
        "#     print(\"Test f1:\",torch.std_mean(torch.Tensor(test_f1)))\n",
        "#     print('----------------------------------------------------------------------------------------------------')\n",
        "\n",
        "#     #   torch.cuda.empty_cache() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0gyOeQX_mzm"
      },
      "source": [
        "###Nested K-fold Cross Valiadtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG5SD48fpD3o"
      },
      "source": [
        "# FOR NESTED CROSS VALIDATION\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "train_val_accuracy,train_val_loss,test_loss,test_accuracy,test_recall,test_precision,test_f1=[],[],[],[],[],[],[]\n",
        "best_learning_rate=[]\n",
        "\n",
        "#Split in 10 folds - 9 folds for training+validation set and 1 fold for test set\n",
        "for outer_cv_count in range(10):\n",
        "\n",
        "  train_val_dataloader=create_dataloader(input_ids[train_valid_ids[outer_cv_count]],\n",
        "                                         attention_masks[train_valid_ids[outer_cv_count]],\n",
        "                                         labels[train_valid_ids[outer_cv_count]])\n",
        "\n",
        "  test_dataloader=create_dataloader(input_ids[test_ids[outer_cv_count]],\n",
        "                                    attention_masks[test_ids[outer_cv_count]],\n",
        "                                    labels[test_ids[outer_cv_count]])\n",
        "\n",
        "  #Check for hyperparameters\n",
        "  learning_rate=[5e-6,1e-5,2e-5,4e-5]\n",
        "\n",
        "  mean_train_loss,mean_train_accuracy=[],[]\n",
        "  mean_validation_loss,mean_validation_accuracy=[],[]\n",
        "\n",
        "  #Enumeratig over all hyperparameter combinations\n",
        "  for i in range(learning_rate.__len__()):\n",
        "\n",
        "    inner_cv_count=0\n",
        "    train_loss,train_accuracy=[],[]\n",
        "    validation_loss,validation_accuracy=[],[]\n",
        "\n",
        "    #Split in 3 folds - 2 folds for train set and 1 fold for validation\n",
        "    for train_id, valid_id in inner_cv.split(input_ids[train_valid_ids[outer_cv_count]]):\n",
        "\n",
        "      train_dataloader=create_dataloader(input_ids[train_id],\n",
        "                                        attention_masks[train_id],\n",
        "                                        labels[train_id])\n",
        "      validation_dataloader=create_dataloader(input_ids[valid_id],\n",
        "                                        attention_masks[valid_id],\n",
        "                                        labels[valid_id])\n",
        "\n",
        "      tr_accuracy,tr_loss,vd_accuracy,vd_loss=train_and_validate_model(learning_rate[i],train_dataloader,validation_dataloader)\n",
        "      print(\"MODEL: {}\".format(inner_cv_count+1),\n",
        "            \"| Train accuracy: {:7.5f}\".format(tr_accuracy),\n",
        "            \"| Train loss: {:7.5f}\".format(tr_loss),\n",
        "            \"| Validation accuracy: {:7.5f}\".format(vd_accuracy),\n",
        "            \"| Validation loss: {:7.5f}\".format(vd_loss))\n",
        "        \n",
        "      train_accuracy.append(tr_accuracy)\n",
        "      train_loss.append(tr_loss)\n",
        "      validation_accuracy.append(vd_accuracy)\n",
        "      validation_loss.append(vd_loss)\n",
        "      inner_cv_count+=1\n",
        "\n",
        "      torch.cuda.empty_cache() \n",
        "\n",
        "    mean_train_accuracy.append(sum(train_accuracy)/3)\n",
        "    mean_train_loss.append(sum(train_loss)/3)\n",
        "    mean_validation_accuracy.append(sum(validation_accuracy)/3)\n",
        "    mean_validation_loss.append(sum(validation_loss)/3)\n",
        "    \n",
        "    print(\"P: \"+str(learning_rate[i])+\n",
        "              \" | Mean train accuracy: {:7.5f}\".format(mean_train_accuracy[i])+\n",
        "              \" | Mean train loss: {:7.5f}\".format(mean_train_loss[i]) +\n",
        "              \" | Mean validation accuracy: {:7.5f}\".format(mean_validation_accuracy[i])+\n",
        "              \" | Mean validation loss: {:7.5f}\".format(mean_validation_loss[i]))\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  best_learning_rate.append(learning_rate[mean_validation_accuracy.index(max(mean_validation_accuracy))])\n",
        "  tr_accuracy,tr_loss,ts_accuracy,ts_loss,ts_precision,ts_recall,ts_f1=train_and_test_model(best_learning_rate[outer_cv_count],train_val_dataloader,test_dataloader)\n",
        "\n",
        "  train_val_accuracy.append(tr_accuracy)    \n",
        "  train_val_loss.append(tr_loss)\n",
        "  test_accuracy.append(ts_accuracy)\n",
        "  test_loss.append(ts_loss)\n",
        "  test_precision.append(ts_precision)\n",
        "  test_recall.append(ts_recall)\n",
        "  test_f1.append(ts_f1)\n",
        "\n",
        "  print(\"Learning Rate: {}\".format(best_learning_rate[outer_cv_count])+\n",
        "         \" | Train accuracy: {:7.5f}\".format(train_val_accuracy[outer_cv_count])+ \n",
        "         \" | Train loss: {:7.5f}\".format(train_val_loss[outer_cv_count]) +\n",
        "         \" | Test accuracy: {:7.5f}\".format(test_accuracy[outer_cv_count])+\n",
        "         \" | Test loss: {:7.5f}\".format(test_loss[outer_cv_count])+\n",
        "         \" | Test precision: {:7.5f}\".format(test_precision[outer_cv_count])+\n",
        "         \" | Test recall: {:7.5f}\".format(test_recall[outer_cv_count])+\n",
        "         \" | Test f1 score: {:7.5f}\".format(test_f1[outer_cv_count]))\n",
        "  print('----------------------------------------------------------------------------------------------------')\n",
        "\n",
        "  torch.cuda.empty_cache() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5NWiQI5muC"
      },
      "source": [
        "print(\"Train accuracy:\",torch.std_mean(torch.Tensor(train_val_accuracy)))\n",
        "print(\"Train loss:\",torch.std_mean(torch.Tensor(train_val_loss)))\n",
        "print(\"Test accuracy:\",torch.std_mean(torch.Tensor(test_accuracy)))\n",
        "print(\"Test loss:\",torch.std_mean(torch.Tensor(test_loss)))\n",
        "print(\"Test precision:\",torch.std_mean(torch.Tensor(test_precision)))\n",
        "print(\"Test recall:\",torch.std_mean(torch.Tensor(test_recall)))\n",
        "print(\"Test f1:\",torch.std_mean(torch.Tensor(test_f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYe8pKNV_uMh"
      },
      "source": [
        "##Result\n",
        "```\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97552 | Train loss: 0.07197 | Test accuracy: 0.87826 | Test loss: 0.38027 | Test precision: 0.80741 | Test recall: 0.87200 | Test f1 score: 0.83846\n",
        "Learning Rate: 2e-05 | Train accuracy: 0.95747 | Train loss: 0.11721 | Test accuracy: 0.88406 | Test loss: 0.33958 | Test precision: 0.79365 | Test recall: 0.87719 | Test f1 score: 0.83333\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97906 | Train loss: 0.06897 | Test accuracy: 0.90725 | Test loss: 0.32077 | Test precision: 0.86607 | Test recall: 0.85088 | Test f1 score: 0.85841\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97713 | Train loss: 0.07445 | Test accuracy: 0.87246 | Test loss: 0.33145 | Test precision: 0.85075 | Test recall: 0.82609 | Test f1 score: 0.83824\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97938 | Train loss: 0.06597 | Test accuracy: 0.89565 | Test loss: 0.37495 | Test precision: 0.83721 | Test recall: 0.87805 | Test f1 score: 0.85714\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97648 | Train loss: 0.07712 | Test accuracy: 0.90725 | Test loss: 0.29939 | Test precision: 0.88136 | Test recall: 0.85246 | Test f1 score: 0.86667\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97390 | Train loss: 0.08141 | Test accuracy: 0.91014 | Test loss: 0.31519 | Test precision: 0.89076 | Test recall: 0.85484 | Test f1 score: 0.87243\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.97552 | Train loss: 0.07366 | Test accuracy: 0.89565 | Test loss: 0.28805 | Test precision: 0.82353 | Test recall: 0.93333 | Test f1 score: 0.87500\n",
        "Learning Rate: 4e-05 | Train accuracy: 0.94459 | Train loss: 0.15971 | Test accuracy: 0.85507 | Test loss: 0.38205 | Test precision: 0.76978 | Test recall: 0.85600 | Test f1 score: 0.81061\n",
        "Learning Rate: 2e-05 | Train accuracy: 0.95427 | Train loss: 0.12953 | Test accuracy: 0.90698 | Test loss: 0.28878 | Test precision: 0.86364 | Test recall: 0.84821 | Test f1 score: 0.85586\n",
        "```\n",
        "Train accuracy: (tensor(0.0124), tensor(0.9693))\n",
        "\n",
        "Train loss: (tensor(0.0320), tensor(0.0920))\n",
        "\n",
        "Test accuracy: (tensor(0.0184), tensor(0.8913))\n",
        "\n",
        "Test loss: (tensor(0.0365), tensor(0.3320))\n",
        "\n",
        "Test precision: (tensor(0.0395), tensor(0.8384))\n",
        "\n",
        "Test recall: (tensor(0.0286), tensor(0.8649))\n",
        "\n",
        "Test f1: (tensor(0.0202), tensor(0.8506))"
      ]
    }
  ]
}