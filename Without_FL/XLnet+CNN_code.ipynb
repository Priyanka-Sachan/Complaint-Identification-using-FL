{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "XLnet+CNN_code",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanka-Sachan/Complaint-Identification-using-FL/blob/master/Without_FL/XLnet%2BCNN_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuwhyl2BhBfh"
      },
      "source": [
        "\n",
        "#CNN with XLNet embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "from transformers import AdamW\n",
        "from transformers import XLNetModel, XLNetTokenizer\n",
        "from transformers import get_scheduler\n",
        "from datasets import load_metric\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jti9LcilqcIc"
      },
      "source": [
        "# For Reproducibility\n",
        "SEED=9\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlJ02Pvwf55f"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTPIeUT7AuAT"
      },
      "source": [
        "##Reading and Pre-processing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ"
      },
      "source": [
        "# Load the .csv file.\n",
        "data = pd.read_csv(\"complaints-data.csv\", header=None, names=['id', 'tweet', 'y', 'industry'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcEAbCxiZOiT"
      },
      "source": [
        "# Tokenize sentences\n",
        "sentences=data.tweet.values\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "MAX_LEN=49\n",
        "tokens= [tokenizer(sentence,add_special_tokens=False, padding='max_length',truncation=\"only_first\", max_length=MAX_LEN) for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeKuiqNao777"
      },
      "source": [
        "# Get input ids, attention masks and labels.\n",
        "input_ids=np.asarray([np.asarray(token['input_ids']) for token in tokens])\n",
        "attention_masks=np.asarray([np.asarray(token['attention_mask']) for token in tokens])\n",
        "labels=data.y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEuWI-05AzNv"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yIbdnOg0wd0"
      },
      "source": [
        "# Getting xlnet model for word embedding\n",
        "xlnet=XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "# Freeze all the parameters\n",
        "for param in xlnet.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YOdShZYdYKG"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n",
        "    def __init__(self,\n",
        "                 xlnet,\n",
        "                 embed_dim=768,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.1):\n",
        "        \"\"\"\n",
        "        The constructor for CNN_NLP class.\n",
        "\n",
        "        Args:\n",
        "            xlnet (XLNetModel): Returns pretrained embeddings with\n",
        "                shape (sentence_length, embed_dim)\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100]\n",
        "            n_classes (int): Number of classes. Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        #embedding layer\n",
        "        self.xlnet = xlnet\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "\n",
        "    def forward(self, ids,masks):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "            masks (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
        "        x_embed = self.xlnet(input_ids=ids,attention_mask=masks)[0].float()\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "        \n",
        "        # Concatenate x_pool_list to feed the fully connected layer.\n",
        "        # Output shape: (b, sum(num_filters))\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],dim=1)\n",
        "        \n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC8nUn2CA1zO"
      },
      "source": [
        "##Training and Testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX47HxYb98ka"
      },
      "source": [
        "# Function to  create dataloader\n",
        "def create_dataloader(input_ids,masks,labels):\n",
        "\n",
        "  input_ids=torch.tensor(input_ids)\n",
        "  masks=torch.tensor(masks)\n",
        "  labels=torch.tensor(labels)\n",
        "\n",
        "  data = TensorDataset(input_ids,masks,labels)\n",
        "  sampler = SequentialSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=32) \n",
        "\n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv029I3s5pN3"
      },
      "source": [
        "#Function to train and validate a model\n",
        "def train_and_validate_model(learning_rate,train_dataloader,validation_dataloader): \n",
        " \n",
        "    model = CNN(xlnet)\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    # optimizer = optim.Adadelta(model.parameters(),lr=learning_rate,rho=0.95)\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "    num_epochs = 4\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "    early_stopping = EarlyStopping(patience=3, verbose=False)\n",
        " \n",
        "    train_accuracy_metric=load_metric(\"accuracy\")\n",
        "    valid_accuracy_metric=load_metric(\"accuracy\")\n",
        " \n",
        "    train_loss,valid_loss=0,0\n",
        "    pr_train_loss,pr_valid_loss=0,0\n",
        " \n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      train_losses = []\n",
        "      valid_losses = []\n",
        " \n",
        "      model.train()\n",
        "      for batch in train_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          logits = model(b_input_ids,b_input_mask)\n",
        "          loss=criterion(logits,b_labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        " \n",
        "          train_losses.append(loss.item())\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          train_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        " \n",
        "      model.eval()\n",
        "      for batch in validation_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          with torch.no_grad():\n",
        "              logits = model(b_input_ids,b_input_mask)\n",
        "          loss=criterion(logits,b_labels)\n",
        " \n",
        "          valid_losses.append(loss.item())\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          valid_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "      pr_train_loss=train_loss\n",
        "      train_loss = np.average(train_losses)\n",
        "      pr_valid_loss=valid_loss\n",
        "      valid_loss = np.average(valid_losses)\n",
        " \n",
        "      early_stopping(valid_loss, model)\n",
        "          \n",
        "      if early_stopping.early_stop:\n",
        "          print(\"Early stopping\")\n",
        "          valid_loss=pr_valid_loss\n",
        "          train_loss=pr_train_loss\n",
        "          break\n",
        "\n",
        "      train_accuracy=train_accuracy_metric.compute()['accuracy']\n",
        "      valid_accuracy=valid_accuracy_metric.compute()['accuracy']\n",
        "\n",
        "      print(\"EPOCH: {}\".format(epoch+1),\n",
        "            \"| Train accuracy: {:7.5f}\".format(train_accuracy),\n",
        "            \"| Train loss: {:7.5f}\".format(train_loss),\n",
        "            \"| Validation accuracy: {:7.5f}\".format(valid_accuracy),\n",
        "            \"| Validation loss: {:7.5f}\".format(valid_loss))\n",
        " \n",
        "    return train_accuracy,train_loss,valid_accuracy,valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5butv1ImLHWZ"
      },
      "source": [
        "#Function to train and test a model\n",
        "def train_and_test_model(learning_rate,train_dataloader,test_dataloader): \n",
        " \n",
        "    model = CNN(xlnet)\n",
        "    model.cuda()\n",
        " \n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    # optimizer = optim.Adadelta(model.parameters(),lr=learning_rate,rho=0.95)\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    \n",
        "    num_epochs = 4\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        " \n",
        "    train_accuracy_metric=load_metric(\"accuracy\")\n",
        "    test_accuracy_metric=load_metric(\"accuracy\")\n",
        "    test_precision_metric=load_metric(\"precision\")\n",
        "    test_recall_metric=load_metric(\"recall\")\n",
        "    test_f1_metric=load_metric(\"f1\")\n",
        "\n",
        "    train_loss,test_loss=0,0\n",
        " \n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      train_losses = []\n",
        "      model.train()\n",
        "      for batch in train_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          logits = model(b_input_ids,b_input_mask)\n",
        "          loss=criterion(logits,b_labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        " \n",
        "          train_losses.append(loss.item())\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          train_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        "\n",
        "      train_accuracy=train_accuracy_metric.compute()['accuracy']\n",
        "      train_loss = np.average(train_losses)\n",
        "      print(\"EPOCH: {}\".format(epoch+1),\n",
        "            \"| Train accuracy: {:7.5f}\".format(train_accuracy),\n",
        "            \"| Train loss: {:7.5f}\".format(train_loss))\n",
        " \n",
        "    test_losses = []\n",
        "    model.eval()\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids,b_input_mask)\n",
        "        loss=criterion(logits,b_labels)\n",
        " \n",
        "        test_losses.append(loss.item())\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        test_accuracy_metric.add_batch(predictions=predictions,references=b_labels)\n",
        "        test_precision_metric.add_batch(predictions=predictions,references=b_labels)\n",
        "        test_recall_metric.add_batch(predictions=predictions,references=b_labels)\n",
        "        test_f1_metric.add_batch(predictions=predictions,references=b_labels)\n",
        " \n",
        "    test_loss = np.average(test_losses)\n",
        "    test_accuracy=test_accuracy_metric.compute()['accuracy']\n",
        "    test_precision=test_precision_metric.compute()['precision']\n",
        "    test_recall=test_recall_metric.compute()['recall']\n",
        "    test_f1=test_f1_metric.compute()['f1']\n",
        "\n",
        "    print(\"Learning Rate: {}\".format(learning_rate)+\n",
        "        \" | Test accuracy: {:7.5f}\".format(test_accuracy)+\n",
        "        \" | Test loss: {:7.5f}\".format(test_loss)+\n",
        "        \" | Test precision: {:7.5f}\".format(test_precision)+\n",
        "        \" | Test recall: {:7.5f}\".format(test_recall)+\n",
        "        \" | Test f1 score: {:7.5f}\".format(test_f1))\n",
        " \n",
        "    return train_accuracy,train_loss,test_accuracy,test_loss,test_precision,test_recall,test_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLO7vaF_A716"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jPLEaBDmLLy"
      },
      "source": [
        "# Prepare cross validation\n",
        "outer_cv = KFold(n_splits=10, shuffle=True, random_state= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e6s4fB8mMCy"
      },
      "source": [
        "train_valid_ids=[]\n",
        "test_ids=[]\n",
        "#Split in 10 folds - 9 folds for training+validation set and 1 fold for test set\n",
        "for train_valid_id, test_id in outer_cv.split(input_ids):\n",
        "  train_valid_ids.append(train_valid_id)\n",
        "  test_ids.append(test_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kG17sHGmSnn"
      },
      "source": [
        "# #To save train_valid_set and test_set iterations of outer cross validation\n",
        "# torch.save(train_valid_ids, 'train_valid_ids.pt')\n",
        "# torch.save(test_ids, 'test_ids.pt')\n",
        "# buffer = io.BytesIO()\n",
        "# torch.save(train_valid_ids, buffer)\n",
        "# torch.save(test_ids, buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4i9N2OwmTc4"
      },
      "source": [
        "# #Loading train_valid_set and test_set\n",
        "# with open('train_valid_ids.pt', 'rb') as f:\n",
        "#   buffer = io.BytesIO(f.read())\n",
        "# train_valid_ids=torch.load(buffer)\n",
        "\n",
        "# with open('test_ids.pt', 'rb') as f:\n",
        "#   buffer = io.BytesIO(f.read())\n",
        "# test_ids=torch.load(buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjKdWikA9lJ"
      },
      "source": [
        "###K-fold Cross Valiadtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mb9cPnmqrDd"
      },
      "source": [
        "# lr=0.001\n",
        "# # FOR CROSS VALIDATION\n",
        "# train_accuracy,train_loss,test_loss,test_accuracy,test_recall,test_precision,test_f1=[],[],[],[],[],[],[]\n",
        "\n",
        "# #Split in 10 folds - 9 folds for training+validation set and 1 fold for test set\n",
        "# for outer_cv_count in range(10):\n",
        "\n",
        "#   train_dataloader=create_dataloader(input_ids[train_valid_ids[outer_cv_count]],\n",
        "#                                          attention_masks[train_valid_ids[outer_cv_count]],\n",
        "#                                          labels[train_valid_ids[outer_cv_count]])\n",
        "\n",
        "#   test_dataloader=create_dataloader(input_ids[test_ids[outer_cv_count]],\n",
        "#                                     attention_masks[test_ids[outer_cv_count]],\n",
        "#                                     labels[test_ids[outer_cv_count]])\n",
        "\n",
        "#   tr_accuracy,tr_loss,ts_accuracy,ts_loss,ts_precision,ts_recall,ts_f1=train_and_test_model(lr,train_dataloader,test_dataloader)    \n",
        "\n",
        "#   train_accuracy.append(tr_accuracy)    \n",
        "#   train_loss.append(tr_loss)\n",
        "#   test_accuracy.append(ts_accuracy)\n",
        "#   test_loss.append(ts_loss)\n",
        "#   test_precision.append(ts_precision)\n",
        "#   test_recall.append(ts_recall)\n",
        "#   test_f1.append(ts_f1)\n",
        "#   print('----------------------------------------------------------------------------------------------------')\n",
        "\n",
        "#   torch.cuda.empty_cache() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9AQ0qHaqq1t"
      },
      "source": [
        "# print(\"Train accuracy:\",torch.std_mean(torch.Tensor(train_accuracy)))\n",
        "# print(\"Train loss:\",torch.std_mean(torch.Tensor(train_loss)))\n",
        "# print(\"Test accuracy:\",torch.std_mean(torch.Tensor(test_accuracy)))\n",
        "# print(\"Test loss:\",torch.std_mean(torch.Tensor(test_loss)))\n",
        "# print(\"Test precision:\",torch.std_mean(torch.Tensor(test_precision)))\n",
        "# print(\"Test recall:\",torch.std_mean(torch.Tensor(test_recall)))\n",
        "# print(\"Test f1:\",torch.std_mean(torch.Tensor(test_f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL0yJ2rKBAwh"
      },
      "source": [
        "###Nested K-fold Cross Valiadtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG5SD48fpD3o"
      },
      "source": [
        "# FOR NESTED CROSS VALIDATION\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "train_val_accuracy,train_val_loss,test_loss,test_accuracy,test_recall,test_precision,test_f1=[],[],[],[],[],[],[]\n",
        "best_learning_rate=[]\n",
        "\n",
        "#Split in 10 folds - 9 folds for training+validation set and 1 fold for test set\n",
        "for outer_cv_count in range(10):\n",
        "\n",
        "  train_val_dataloader=create_dataloader(input_ids[train_valid_ids[outer_cv_count]],\n",
        "                                         attention_masks[train_valid_ids[outer_cv_count]],\n",
        "                                         labels[train_valid_ids[outer_cv_count]])\n",
        "\n",
        "  test_dataloader=create_dataloader(input_ids[test_ids[outer_cv_count]],\n",
        "                                    attention_masks[test_ids[outer_cv_count]],\n",
        "                                    labels[test_ids[outer_cv_count]])\n",
        "\n",
        "  #Check for hyperparameters\n",
        "  learning_rate=[ 3e-4,5e-4,1e-3,3e-3 ]\n",
        "\n",
        "  mean_train_loss,mean_train_accuracy=[],[]\n",
        "  mean_validation_loss,mean_validation_accuracy=[],[]\n",
        "\n",
        "  #Enumeratig over all hyperparameter combinations\n",
        "  for i in range(learning_rate.__len__()):\n",
        "\n",
        "    inner_cv_count=0\n",
        "    train_loss,train_accuracy=[],[]\n",
        "    validation_loss,validation_accuracy=[],[]\n",
        "\n",
        "    #Split in 3 folds - 2 folds for train set and 1 fold for validation\n",
        "    for train_id, valid_id in inner_cv.split(input_ids[train_valid_ids[outer_cv_count]]):\n",
        "\n",
        "      train_dataloader=create_dataloader(input_ids[train_id],\n",
        "                                        attention_masks[train_id],\n",
        "                                        labels[train_id])\n",
        "      validation_dataloader=create_dataloader(input_ids[valid_id],\n",
        "                                        attention_masks[valid_id],\n",
        "                                        labels[valid_id])\n",
        "\n",
        "      tr_accuracy,tr_loss,vd_accuracy,vd_loss=train_and_validate_model(learning_rate[i],train_dataloader,validation_dataloader)\n",
        "      print(\"MODEL: {}\".format(inner_cv_count+1),\n",
        "            \"| Train accuracy: {:7.5f}\".format(tr_accuracy),\n",
        "            \"| Train loss: {:7.5f}\".format(tr_loss),\n",
        "            \"| Validation accuracy: {:7.5f}\".format(vd_accuracy),\n",
        "            \"| Validation loss: {:7.5f}\".format(vd_loss))\n",
        "        \n",
        "      train_accuracy.append(tr_accuracy)\n",
        "      train_loss.append(tr_loss)\n",
        "      validation_accuracy.append(vd_accuracy)\n",
        "      validation_loss.append(vd_loss)\n",
        "      inner_cv_count+=1\n",
        "\n",
        "      torch.cuda.empty_cache() \n",
        "\n",
        "    mean_train_accuracy.append(sum(train_accuracy)/3)\n",
        "    mean_train_loss.append(sum(train_loss)/3)\n",
        "    mean_validation_accuracy.append(sum(validation_accuracy)/3)\n",
        "    mean_validation_loss.append(sum(validation_loss)/3)\n",
        "    \n",
        "    print(\"P: \"+str(learning_rate[i])+\n",
        "              \" | Mean train accuracy: {:7.5f}\".format(mean_train_accuracy[i])+\n",
        "              \" | Mean train loss: {:7.5f}\".format(mean_train_loss[i]) +\n",
        "              \" | Mean validation accuracy: {:7.5f}\".format(mean_validation_accuracy[i])+\n",
        "              \" | Mean validation loss: {:7.5f}\".format(mean_validation_loss[i]))\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  best_learning_rate.append(learning_rate[mean_validation_accuracy.index(max(mean_validation_accuracy))])\n",
        "  tr_accuracy,tr_loss,ts_accuracy,ts_loss,ts_precision,ts_recall,ts_f1=train_and_test_model(best_learning_rate[outer_cv_count],train_val_dataloader,test_dataloader)\n",
        "\n",
        "  train_val_accuracy.append(tr_accuracy)    \n",
        "  train_val_loss.append(tr_loss)\n",
        "  test_accuracy.append(ts_accuracy)\n",
        "  test_loss.append(ts_loss)\n",
        "  test_precision.append(ts_precision)\n",
        "  test_recall.append(ts_recall)\n",
        "  test_f1.append(ts_f1)\n",
        "\n",
        "  print(\"Learning Rate: {}\".format(best_learning_rate[outer_cv_count])+\n",
        "         \" | Train accuracy: {:7.5f}\".format(train_val_accuracy[outer_cv_count])+ \n",
        "         \" | Train loss: {:7.5f}\".format(train_val_loss[outer_cv_count]) +\n",
        "         \" | Test accuracy: {:7.5f}\".format(test_accuracy[outer_cv_count])+\n",
        "         \" | Test loss: {:7.5f}\".format(test_loss[outer_cv_count])+\n",
        "         \" | Test precision: {:7.5f}\".format(test_precision[outer_cv_count])+\n",
        "         \" | Test recall: {:7.5f}\".format(test_recall[outer_cv_count])+\n",
        "         \" | Test f1 score: {:7.5f}\".format(test_f1[outer_cv_count]))\n",
        "  print('----------------------------------------------------------------------------------------------------')\n",
        "\n",
        "  torch.cuda.empty_cache() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5NWiQI5muC"
      },
      "source": [
        "print(\"Train accuracy:\",torch.std_mean(torch.Tensor(train_val_accuracy)))\n",
        "print(\"Train loss:\",torch.std_mean(torch.Tensor(train_val_loss)))\n",
        "print(\"Test accuracy:\",torch.std_mean(torch.Tensor(test_accuracy)))\n",
        "print(\"Test loss:\",torch.std_mean(torch.Tensor(test_loss)))\n",
        "print(\"Test precision:\",torch.std_mean(torch.Tensor(test_precision)))\n",
        "print(\"Test recall:\",torch.std_mean(torch.Tensor(test_recall)))\n",
        "print(\"Test f1:\",torch.std_mean(torch.Tensor(test_f1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KPQhScYBlxg"
      },
      "source": [
        "##Result\n",
        "```\n",
        "Learning Rate: 0.001  | Train accuracy: 0.82925 | Train loss: 0.38794 | Test accuracy: 0.81739 | Test loss: 0.36239 | Test precision: 0.82979 | Test recall: 0.62400 | Test f1 score: 0.71233\n",
        "Learning Rate: 0.001  | Train accuracy: 0.82281 | Train loss: 0.39299 | Test accuracy: 0.85217 | Test loss: 0.35149 | Test precision: 0.82474 | Test recall: 0.70175 | Test f1 score: 0.75829\n",
        "Learning Rate: 0.0005 | Train accuracy: 0.80735 | Train loss: 0.44210 | Test accuracy: 0.85217 | Test loss: 0.36510 | Test precision: 0.84615 | Test recall: 0.67544 | Test f1 score: 0.75122\n",
        "Learning Rate: 0.001  | Train accuracy: 0.82700 | Train loss: 0.38920 | Test accuracy: 0.80290 | Test loss: 0.40414 | Test precision: 0.88043 | Test recall: 0.58696 | Test f1 score: 0.70435\n",
        "Learning Rate: 0.0005 | Train accuracy: 0.82152 | Train loss: 0.42620 | Test accuracy: 0.83188 | Test loss: 0.38299 | Test precision: 0.79817 | Test recall: 0.70732 | Test f1 score: 0.75000\n",
        "Learning Rate: 0.001  | Train accuracy: 0.82893 | Train loss: 0.39580 | Test accuracy: 0.82609 | Test loss: 0.38001 | Test precision: 0.85227 | Test recall: 0.61475 | Test f1 score: 0.71429\n",
        "Learning Rate: 0.001  | Train accuracy: 0.82023 | Train loss: 0.39942 | Test accuracy: 0.84638 | Test loss: 0.37946 | Test precision: 0.85149 | Test recall: 0.69355 | Test f1 score: 0.76444\n",
        "Learning Rate: 0.001  | Train accuracy: 0.83537 | Train loss: 0.39628 | Test accuracy: 0.79420 | Test loss: 0.40989 | Test precision: 0.82653 | Test recall: 0.60000 | Test f1 score: 0.69528\n",
        "Learning Rate: 0.0005 | Train accuracy: 0.81830 | Train loss: 0.42008 | Test accuracy: 0.81739 | Test loss: 0.39898 | Test precision: 0.82292 | Test recall: 0.63200 | Test f1 score: 0.71493\n",
        "Learning Rate: 0.001  | Train accuracy: 0.81707 | Train loss: 0.39818 | Test accuracy: 0.84884 | Test loss: 0.34167 | Test precision: 0.81250 | Test recall: 0.69643 | Test f1 score: 0.75000\n",
        "```\n",
        "Train accuracy: (tensor(0.0079), tensor(0.8228))\n",
        "\n",
        "Train loss: (tensor(0.0182), tensor(0.4048))\n",
        "\n",
        "Test accuracy: (tensor(0.0209), tensor(0.8289))\n",
        "\n",
        "Test loss: (tensor(0.0226), tensor(0.3776))\n",
        "\n",
        "Test precision: (tensor(0.0235), tensor(0.8345))\n",
        "\n",
        "Test recall: (tensor(0.0463), tensor(0.6532))\n",
        "\n",
        "Test f1: (tensor(0.0255), tensor(0.7315))"
      ]
    }
  ]
}